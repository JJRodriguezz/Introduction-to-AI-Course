{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trOQNDiPb7dG",
        "outputId": "023e21e5-3228-452c-f83d-cfb6ab017576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q después del entrenamiento:\n",
            "[[0.48890282 0.72700166]\n",
            " [0.43234778 0.80953178]\n",
            " [0.60298176 0.89984602]\n",
            " [0.67012562 0.99997344]\n",
            " [0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # manejo de la tabla Q\n",
        "import random #Elección de acciones aleatorias\n",
        "\n",
        "num_states = 5  # Número de estados\n",
        "num_actions = 2  # Número de acciones: 0 = izquierda, 1 = derecha\n",
        "\n",
        "# Parámetros del algoritmo de Q-learning\n",
        "q_table = np.zeros((num_states, num_actions))  # Tabla Q inicializada en ceros\n",
        "alpha = 0.1      # Tasa de aprendizaje\n",
        "gamma = 0.9      # Factor de descuento\n",
        "epsilon = 0.3    # Parámetro de exploración\n",
        "\n",
        "# Definición de recompensas y estado terminal\n",
        "goal_state = 4   # Estado meta\n",
        "rewards = np.zeros(num_states) #definición de un vector de recompensas para cada estado\n",
        "rewards[goal_state] = 1  # Recompensa de +1 en el estado meta\n",
        "\n",
        "# Algoritmo de Q-learning\n",
        "num_episodes = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = 0  # Estado inicial\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Decidir si explorar o explotar\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, num_actions - 1)  # Explorar: elegir una acción aleatoria\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Explotar: elegir la mejor acción basada en Q-table\n",
        "\n",
        "        # Tomar la acción y observar el nuevo estado y la recompensa\n",
        "        if action == 1 and state!=4:\n",
        "          new_state = state + 1\n",
        "        elif action == 1 and state==4:\n",
        "          new_state = state\n",
        "        else:\n",
        "          new_state=max(0, state - 1)  # Moverse a la derecha o izquierda\n",
        "        reward = rewards[new_state]\n",
        "\n",
        "        # Actualizar la Q-table usando la fórmula de Q-learning\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[new_state])\n",
        "\n",
        "        q_table[state, action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "        # Actualizar el estado actual\n",
        "        state = new_state\n",
        "\n",
        "        # Terminar si se ha alcanzado el estado meta\n",
        "        if state == goal_state:\n",
        "            done = True\n",
        "\n",
        "# Mostrar la tabla Q entrenada\n",
        "print(\"Tabla Q después del entrenamiento:\")\n",
        "print(q_table)"
      ]
    }
  ]
}